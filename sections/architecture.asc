
[NOTE]
As a bioinformatician developing a programming language and type system,
I quickly saw the similarities between "conventional" type systems and
ontologies. Ontologies are commonly used in biology to formally describe the
domain-specific relationships, for example the functions of genes. To me, a
type system was a method for formally describing what a function _is_, thus it
seemed to play the same role as an ontology. So I came up with the idea of a
_semantic_ type system; one based on formal logic, specifically description
logic, rather than type theory. Later I learned of the correspondence between
lambda calculus-based type theory and formal logic, and I began to see that my
semantic type system was not so radical. I have yet to work out exactly what I
want my _semantic_ type system to do and how it relates to more conventional
type systems. My current type system is supports generic and parameterized
types (system F generalized for multiple langauges).

[NOTE]
The following material is all crap from my old SBIR grant. Most of it is
dreadfully out of date.

`morloc` is an ontology-based workflow framework that uses a code-generating
compiler to integrate components. While the entire system is quite unique, the
components are built on well-established technology.  The system can be divided
into the five layers as show in Fig 5. Work related to the semantic, workflow,
and interoperability layers, where the key technical innovations of `morloc`
occur, is discussed below.

The semantic layer consists of a type system based on the knowledge
representation logic that underlies the semantic web. Knowledge representation
systems, specifically ontologies, have been used extensively across fields to
curate human knowledge. This knowledge can be understood by both humans and
machines. One example of a successful ontology is the EDAM ontology of
bioinformatics operations, data types and identifiers.7 Lamprecht, et al. used
this ontology as a guiding logic for automatic workflow composition program
where the components of the workflow were web services with inputs and outputs
described by EDAM data types.8 Ontologies have also been studied as a tool for
managing databases of functions.9 `morloc` will make heavy use of existing
ontology tools and, where possible, reuse or adapt existing domain ontologies.
`morloc` also will allow constraints to be placed on types, including functions.
This has been explored extensively, for example in the dependently typed
language Idris.10

The workflow layer takes a graph of components and integrates them into a
single program.  There are many systems designed to integrate standalone
services, applications, and functions. For the UNIX community, the pipeline
pattern of sending data through series of specialized filters is common.
Scripting languages are used to coordinate system tools, where data is passed
between the tools as raw files. For the research community, there are many
large scientific platforms that manage workflows and curate resources, for
example Galaxy builds workflows from UNIX scripts with XML metadata11 and
Taverna similarly assembles web services into workflows.12 There are also many
small language-specific libraries for building workflows from external tools or
language constructs. API frameworks like MuleSoft are designed to allow many
web microservices to communicate through a REST interface. The Common Workflow
Language (CWL) is a specification language for describing command line tools
and their composition into workflows.13 CWL scripts can be used to specify
dependencies and automate the lifting of a pipeline into a high-performance
computing context.

The interoperability layer generates the interfaces needed to pass data between
functions of different languages. Interfaces between languages, which allow
calls to foreign languages with little overhead, have been developed for
specific pairs of languages, usually from a high-level language (e.g. Python)
to a high-performance compiled language (usually C) or for select groups of
languages (e.g. GraalVM). The planned `morloc` implementation will take a more
general approach to interoperability: serialize the output of the first
function into a language-agnostic format and then deserialize this in the
second function. The serialization format for the `morloc` prototype will be the
web standard JSON. In the future, we will explore faster industrial systems,
such as Google’s Protocol Buffers.

== The Semantic Type System

The semantic type system is the foundation of `morloc`. The system is based on
description logic, which is widely used across disciplines to organize
knowledge into ontologies, which are directed graphs describing relationships
between concepts. In this section, we will introduce 1) function type
signatures, 2) data signatures, 3) ontologies of types, and 4) how abstract
`morloc` types are converted into concrete language-specific types.

=== Function type signatures with constraints

Here is an example of a `morloc` type signature:

[source]
----
		sample :: i:Integer, xs:[a] -> ys:[a] where {
    i >= 0 
    length(ys) == min(i, length(xs))
}
----

This signature describes a function named sample that returns i random elements
from the vector xs as a new vector ys. The vectors, [a], contain elements of
generic type a, which can be anything. The where clause introduces a list of
constraints. First, at least 0 elements must be sampled. Second, the length of
the output vector must be the minimum of i and the length of xs. These two
constraints describe the edge conditions of the function. Sampling 0 elements
is legal and will return a vector of 0 length. Likewise, if the number of
elements sampled is greater than the length of the input vector, the output
vector will have the same length as the input vector.


=== Data signatures with constraints
Non-function types may also have constraints, for example, counting number
could be defined as:

[source]
----
		CountingNumber :: i:Integer where { i > 0 }
----

While our first prototypes will use constraints only to generate runtime assert
statements in the code, there are at least two powerful options for future
development: 1) developing formal methods for static analysis of constraints
across a pipeline and 2) using these constraints to automate testing of
functions.

=== Ontologies of types

The `morloc` type system is intended to powerfully describe concrete ideas.
Knowledge representation systems, specifically ontologies, are the most natural
system for this purpose. `morloc` will support adding arbitrary relations to
types. Support for special handling for specific relations may be
built into the `morloc` compiler or added as extensions. We have not chosen a
syntax for expressing semantic relations. One option would be to use an
existing W3C standard, such as Turtle format. Alternatively, we could develop
our own syntax. The `morloc` compiler will convert a `morloc` script into a list of
logical statements (an RDF triple store). Typechecking will involve proving the
consistency of the resulting logical system.

The relations between types will be used to encode domain specific handling
into the `morloc` type system. For example, a programmer may state that Meter and
Centimeter are both Length and that they can be interconverted. The function
performing the interconversion will have the signature Meter->Centimeter and
the role convert. If a function that expects the type Meter is
passed data of type Centimeter, an appropriate conversion will be automatically
performed.

=== Mapping from abstract `morloc` type to concrete, language-specific type

Mapping from general `morloc` types, like Integer or [Integer] (a list of
integers) to concrete types in specific languages is one of the basic functions
of the `morloc` compiler. For a given general type, there may be many possible
concrete instances in every language (see <<serialization, the figure below>>).
To convert between concrete instances, the data must first be resolved into a
nested structure of primitive values, clearing away language-specific features.
This structure can then be unambiguously serialized into JSON, a nested
language-agnostic data format.  Next the JSON intermediate form is deserialized
into a nested structure of primitive values in language 2. Finally, this
structure is cast into a language-specific concrete instance. The casting steps
will not always be needed and there will often be reasonable defaults. The
wrappers `morloc` generates around the pure functions will handle casting
operations.

Specifying cast functions will be most important for object oriented languages.
In these cases, the primitive nested structure will hold all the information
needed to build the required object, and the cast function will pass the
appropriate data to the class constructor. We will explore object handling as
we implement support for `C++`.

[#serialzation]
image:serialization.png[]

== Architecture

The `morloc` architecture can divided into 5 layers. The input layer handles the
conversion of input source code into `morloc` packages. It extracts any `morloc`
metadata that is encoded in the source code. This metadata includes the
language-specific type information that may be needed when casting complex
types between languages. The metadata would be stored within the comments above
a function, as is done in Javadocs or Doxygen. 

The interoperability layer involves specification of the language-agnostic data
formats and serializations to/from these formats. The semantic layer includes
knowledge engineering and handles parsing of the type strings, inferencing, and
type checking workflows. The workflow layer takes the raw functions and
workflow graph and generates an executable and user interface. The distribution
layer involves the database that stores all the functions and ontologies and
the algorithms used for accessing them. The user layer includes all the ways in
which a user interacts with `morloc`.

The language grammar specification template is a vital piece of the `morloc`
architecture that will allow support for new languages to be added without
having to modify the core compiler. The `morloc` code generator needs to know
enough of the language grammar to generate wrappers. For example, it needs to
know the syntax for a function call given a list of arguments. In the input
layer, the source code preprocessor needs to know how metadata are encoded in
the source language (i.e. a regular expression that captures a specific pattern
within the source code comments). We have explored these ideas in our current
prototypes, but they will need to be formalized and generalized in order to
realize our goal of easily adding support for almost any language.

== Performance Optimizations and High-Performance Computing

Since `morloc` is built for data science, performance is a top priority. There
are two main performance concerns: serialization and runtime transitions. In
the most naive implementation of the `morloc` compiler, every function would be
an independent script and every function call would require starting up the
runtime (for non-compiled languages) and a serialization/deserialization step.
An obvious optimization, and one that is already implemented in the current
prototypes, is to merge all functions of the same language into the same
script, removing all serialization and startup costs for within-language calls.
But further optimizations are possible. Serialization costs can be reduced by
using an industrial strength serialization solution (such as Google’s Protocol
Buffers), rather than JSON. Existing solutions for efficient between-language
calls also exist for some languages, and it may be possible to automatically
generate the boilerplate to apply these solutions. Startup costs can also be
removed for some interpreted languages by using a server/client interface (e.g.
RServe for R).

Beyond inter-language and serialization optimizations, lifting `morloc` workflows
into high-performance computing frameworks is essential to scaling. One option
is building Common Workflow Language (CWL) specifications for workflows during
the compilation process. This would open the whole suite of CWL compatible
tools and frameworks to `morloc` workflows. One such framework is Arvados
(https://arvados.org/), which could be leveraged to allow `morloc` workflows to
run on local HPC clusters or in the cloud.

In the long-term, we anticipate that one of the most powerful features of
`morloc` will be its ability to automatically generate optimal solutions from
simple components. The `morloc` compiler is given source code for each component,
semantic knowledge about how the components work, and a vast knowledge base
including past performance for all components both individually and when
integrated into workflows. The only limit on the optimization of the compiled
programs is the intelligence of the compiler and the quality of the knowledge
we make available to it.

Given the current acceleration of machine intelligence, we believe the best and
most lasting frameworks will be those that make knowledge and tools available
to the machine. This is exactly what we hope to achieve with `morloc`: to unify
access to programmatic tools and create an elegant system for encoding
knowledge about their operation.

