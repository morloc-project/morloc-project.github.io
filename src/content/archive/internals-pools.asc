=== Pool Architecture

Pools are the runtime workers that actually execute user functions. Each pool is
generated by the compiler from a language-specific template, with user functions
and dispatch tables injected into marked sections. This section describes how
pools work internally.


==== Common structure

All pools share the same fundamental pattern:

1. **Startup**: Receive `socket_path`, `tmpdir`, and `shm_basename` as
   command-line arguments. Initialize shared memory. Start listening on a Unix
   domain socket.
2. **Event loop**: Accept connections, read packets from clients, dispatch to
   the appropriate handler.
3. **Dispatch**: Check the packet type -- ping, local call, or remote call --
   and route accordingly.
4. **Shutdown**: On SIGTERM, set a shutdown flag, finish current work, drain the
   queue, and exit.

The compiler injects code into each pool template at `<<<BREAK>>>` markers:

* **Includes** -- Language-specific imports for user modules
* **Serialization** -- `toAnything`/`fromAnything` specializations for each type
  used in the program
* **Signatures** -- Forward declarations of user functions
* **Manifolds** -- Wrapper functions that deserialize arguments, call the user
  function, and serialize the result
* **Dispatch tables** -- `local_dispatch` and `remote_dispatch` functions that
  map manifold IDs to the appropriate manifold wrapper


==== {cpp} pool

The {cpp} pool (`pool.cpp`) uses a **thread pool with a job queue**. The
architecture:

[source]
----
Main thread:
  - Listens on Unix socket (wait_for_client_with_timeout)
  - Pushes accepted client fds onto a job queue
  - Monitors g_busy_count for dynamic worker spawning

Worker threads (1 initially, grows as needed):
  - Pop client fds from the job queue
  - Read packet from client (stream_from_client)
  - Call dispatch() to handle the packet
  - Send result back to client
  - Close client fd
----

The `dispatch()` function checks the packet type:

* **Ping** -> Return a pong packet immediately
* **Local call** -> Extract the manifold ID and arguments, call
  `local_dispatch(mid, args)` which is a generated switch statement mapping IDs
  to manifold functions
* **Remote call** -> Same, but uses `remote_dispatch(mid, args)` for functions
  called from other pools

Each manifold function follows this pattern:

[source,cpp]
----
uint8_t* manifold_42(const uint8_t** args) {
    // Deserialize arguments from voidstar
    auto x = _get_value<int>(args[0], "i4");
    auto y = _get_value<std::string>(args[1], "s");

    // Call the user function
    auto result = userFunction(x, y);

    // Serialize result to voidstar
    return _put_value(result, "ai4");
}
----

The `_get_value` template extracts a native {cpp} type from a data packet using
the schema string. The `_put_value` template converts a native type to voidstar
(in shared memory), wraps it in a data packet, and returns it.


==== Python pool

The Python pool (`pool.py`) uses a **multiprocessing model** with file
descriptor passing. The architecture:

[source]
----
Main process:
  - Monitors wake-up pipe for busy-worker signals
  - Spawns new workers when all are busy
  - Reaps idle workers that have exited

Listener process:
  - Listens on Unix socket (morloc.wait_for_client)
  - Passes client fds to workers via SCM_RIGHTS over a socketpair

Worker processes (1 initially, grows as needed):
  - Receive client fds via SCM_RIGHTS
  - Call run_job(client_fd) to dispatch
  - Exit after 5 seconds of idle time
----

The listener and workers communicate through a Unix socketpair. When the
listener accepts a new connection, it sends the file descriptor to one of the
worker processes using `SCM_RIGHTS` ancillary data. Workers blocked in
`recvmsg` wake up to receive the fd -- this ensures that only idle workers pick
up new jobs.

Python workers call `run_job()` which, like {cpp}'s `dispatch()`, checks
the packet type and routes to the appropriate handler. The generated `dispatch`
dictionary maps manifold IDs to Python functions that handle deserialization
and serialization through the `pymorloc` C extension.


==== Dynamic worker spawning

Both {cpp} and Python pools start with a single worker and grow dynamically.
The mechanism prevents deadlocks that can occur during foreign calls: if a pool
has one worker and that worker makes a foreign call to another pool, which in
turn calls back, the original pool's worker is blocked and can't handle the
callback.

**{cpp}**: Uses atomic counters `g_busy_count` and `g_total_threads`. When
`foreign_call()` is entered, `g_busy_count` is incremented. The main thread
checks after each `accept()` whether `g_busy_count >= g_total_threads`. If so,
it spawns a new `pthread` worker.

**Python**: Uses shared `multiprocessing.RawValue` counters and a wake-up pipe.
The `foreign_call` function is monkey-patched to increment `busy_count` before
the call and decrement after. When all workers are busy, a byte is written to
the wake-up pipe, signaling the main process to spawn a new worker. Idle workers
exit after `WORKER_IDLE_TIMEOUT` (5 seconds) to avoid resource waste.


==== Foreign calls

A "foreign call" is a cross-language function call between pools. For example,
a {cpp} function that calls a Python function. The calling pool:

1. Serializes each argument into a data packet (voidstar in shared memory,
   with a relative pointer in the packet)
2. Builds a call packet with the target manifold ID and the argument packets
   as payload
3. Sends the call packet over a Unix domain socket to the target pool
4. Waits for a data packet response

[mermaid]
....
sequenceDiagram
    participant CppPool as C++ Pool
    participant PyPool as Python Pool

    CppPool->>CppPool: Execute manifold_7
    Note over CppPool: Needs Python function

    CppPool->>CppPool: Serialize args to voidstar
    CppPool->>CppPool: Build call packet (mid=12)
    CppPool->>CppPool: g_busy_count++

    CppPool->>PyPool: call packet over Unix socket
    PyPool->>PyPool: Dispatch to manifold_12
    PyPool->>PyPool: Deserialize args from voidstar
    PyPool->>PyPool: Call Python function
    PyPool->>PyPool: Serialize result to voidstar

    PyPool-->>CppPool: data packet (relptr to result)

    CppPool->>CppPool: g_busy_count--
    CppPool->>CppPool: Deserialize result
    CppPool->>CppPool: Continue manifold_7
....

The socket path for foreign calls is constructed as `<tmpdir>/<socket_name>`,
where `socket_name` comes from the manifest's pool entry. The {cpp} pool stores
the `tmpdir` globally and builds the full path at call time.


==== Startup sequence

When the nexus needs to run a command, it:

1. Determines which pools are needed (from the command's `needed_pools` list)
2. Calls `fork()` + `execvp()` for each pool, passing:
   - The pool executable (e.g., `python3 pools/pool.py` or `pools/pool-cpp.out`)
   - The Unix socket path (e.g., `/tmp/morloc.XXXXXX/pipe-python3`)
   - The tmpdir path
   - The shared memory basename
3. Each child calls `setpgid(0, 0)` to become its own process group leader
4. The nexus pings each pool with exponential backoff until it responds

Each pool, on startup:

1. Calls `start_daemon()` which creates the Unix domain socket and maps the
   shared memory
2. Begins accepting connections in its event loop
3. The first thing it handles is the nexus's ping -- responding with a pong
   signals readiness


==== Shutdown sequence

Shutdown is triggered by SIGTERM (sent by the nexus) or program completion:

**{cpp} pool**:

1. SIGTERM handler sets the `shutting_down` flag
2. Worker threads notice the flag in `job_queue_pop` (which uses
   `pthread_cond_timedwait` with 100ms timeout) and call `pthread_exit`
3. Main thread broadcasts on the condition variable, then joins all workers
4. Remaining jobs in the queue are drained (client fds closed)
5. The daemon socket is closed and unlinked

**Python pool**:

1. SIGTERM handler sets `shutdown_flag.value = True` and writes to the wake-up
   pipe
2. The main loop exits, closes the wake-up pipe and spare socket fd
3. The listener process is terminated (SIGTERM, then SIGKILL after 1ms)
4. Worker processes are killed and joined

**Nexus cleanup**:

1. Sends SIGTERM to each pool's process group (`kill(-pgid, SIGTERM)`)
2. Polls each group for up to 500ms (50 attempts, 10ms apart)
3. Sends SIGKILL to any surviving groups
4. Does a final `waitpid(-1, WNOHANG)` to reap stragglers
5. Deletes the tmpdir and unlinks all shared memory volumes
